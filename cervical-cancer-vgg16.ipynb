{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport glob\nimport plotly.graph_objects as go\nimport cv2\nfrom PIL import Image\nfrom PIL import ImageFile\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten , Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:06.450416Z","iopub.execute_input":"2021-12-11T07:51:06.4507Z","iopub.status.idle":"2021-12-11T07:51:06.458141Z","shell.execute_reply.started":"2021-12-11T07:51:06.45067Z","shell.execute_reply":"2021-12-11T07:51:06.457112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Dataset ","metadata":{}},{"cell_type":"code","source":"# get the data for training\nroot_dir = '../input'\nintel_dir = '../input/intel-mobileodt-cervical-cancer-screening'\n\ntype1_dir = os.path.join(root_dir, 'type2additional','Typ_1add')\ntype2_dir = os.path.join(root_dir, 'type2add','Type_2')\ntype3_dir = os.path.join(root_dir, 'type3addkjanjkanajnnkjhjj','Type_3_add')\n\ntrain_type1_files = glob.glob(type1_dir+'/*.jpg')\ntrain_type2_files = glob.glob(type2_dir+'/*.jpg')\ntrain_type3_files = glob.glob(type3_dir+'/*.jpg')\n\nadded_type1_files  =  glob.glob(os.path.join(intel_dir, \"additional_Type_1_v2\", \"Type_1\")+'/*.jpg')\nadded_type2_files  =  glob.glob(os.path.join(intel_dir, \"additional_Type_2_v2\", \"Type_2\")+'/*.jpg')\nadded_type3_files  =  glob.glob(os.path.join(intel_dir, \"additional_Type_3_v2\", \"Type_3\")+'/*.jpg')\n\n\n\ntype1_files = train_type1_files\ntype2_files = train_type2_files\ntype3_files = train_type3_files\n\nprint('''Type 1 files for training: {len(type1_files)} \nType 2 files for training: {len(type2_files)}\nType 3 files for training: {len(type3_files)}''' )","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:08.080022Z","iopub.execute_input":"2021-12-11T07:51:08.080386Z","iopub.status.idle":"2021-12-11T07:51:09.401771Z","shell.execute_reply.started":"2021-12-11T07:51:08.080351Z","shell.execute_reply":"2021-12-11T07:51:09.400975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"check folder wise for bad files ","metadata":{}},{"cell_type":"code","source":"#train_type1_files\n# create dataframe of file and labels\ntrain_type1_files = {'filepath': type1_files,\n          'label': ['Type 1']* len(type1_files)}\n\ntrain_type1_df = pd.DataFrame(train_type1_files).sample(frac=1, random_state= 1).reset_index(drop=True)\nlen(train_type1_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:09.403423Z","iopub.execute_input":"2021-12-11T07:51:09.403696Z","iopub.status.idle":"2021-12-11T07:51:09.418664Z","shell.execute_reply.started":"2021-12-11T07:51:09.40366Z","shell.execute_reply":"2021-12-11T07:51:09.417712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Individually checked each directory and corrected the corrupted files to avoid error \"premature ending of jpeg file\"","metadata":{}},{"cell_type":"markdown","source":"### since the error was still coming, deleted the files identified manually.","metadata":{}},{"cell_type":"code","source":"dir_path = r'../input/type2additional/Typ_1add'","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:11.038774Z","iopub.execute_input":"2021-12-11T07:51:11.039034Z","iopub.status.idle":"2021-12-11T07:51:11.043558Z","shell.execute_reply.started":"2021-12-11T07:51:11.039006Z","shell.execute_reply":"2021-12-11T07:51:11.041644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_and_fix(img_path, img_name):\n    # detect for premature ending\n    try:\n        with open( img_path, 'rb') as im :\n            im.seek(-2,2)\n            if im.read() == b'\\xff\\xd9':\n                print('Image OK :', img_name) \n            else: \n                # fix image\n                img = cv2.imread(img_path)\n                cv2.imwrite( img_path, img)\n                print('FIXED corrupted image :', img_name)           \n    except(IOError, SyntaxError) as e :\n      print(e)\n      print(\"Unable to load/write Image : {} . Image might be destroyed\".format(img_path) )\n\n\nfor path in os.listdir(dir_path):\n    # Make sure to change the extension if it is nor 'jpg' ( for example 'JPG','PNG' etc..)\n    if path.endswith('.jpg'):\n      img_path = os.path.join(dir_path, path)\n      detect_and_fix( img_path=img_path, img_name = path)\n        \n\nprint(\"Process Finished\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:11.761609Z","iopub.execute_input":"2021-12-11T07:51:11.762149Z","iopub.status.idle":"2021-12-11T07:51:14.287912Z","shell.execute_reply.started":"2021-12-11T07:51:11.76211Z","shell.execute_reply":"2021-12-11T07:51:14.28717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_path = r'../input/type2add/Type_2'\n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:14.289846Z","iopub.execute_input":"2021-12-11T07:51:14.290097Z","iopub.status.idle":"2021-12-11T07:51:14.294197Z","shell.execute_reply.started":"2021-12-11T07:51:14.290062Z","shell.execute_reply":"2021-12-11T07:51:14.293284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_and_fix(img_path, img_name):\n    # detect for premature ending\n    try:\n        with open( img_path, 'rb') as im :\n            im.seek(-2,2)\n            if im.read() == b'\\xff\\xd9':\n                print('Image OK :', img_name) \n            else: \n                # fix image\n                img = cv2.imread(img_path)\n                cv2.imwrite( img_path, img)\n                print('FIXED corrupted image :', img_name)           \n    except(IOError, SyntaxError) as e :\n      print(e)\n      print(\"Unable to load/write Image : {} . Image might be destroyed\".format(img_path) )\n\n\nfor path in os.listdir(dir_path):\n    # Make sure to change the extension if it is nor 'jpg' ( for example 'JPG','PNG' etc..)\n    if path.endswith('.jpg'):\n      img_path = os.path.join(dir_path, path)\n      detect_and_fix( img_path=img_path, img_name = path)\n        \n\nprint(\"Process Finished\")","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:14.295681Z","iopub.execute_input":"2021-12-11T07:51:14.296315Z","iopub.status.idle":"2021-12-11T07:51:16.040854Z","shell.execute_reply.started":"2021-12-11T07:51:14.296264Z","shell.execute_reply":"2021-12-11T07:51:16.03999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_path = r'../input/type3addkjanjkanajnnkjhjj/Type_3_add'","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:16.042719Z","iopub.execute_input":"2021-12-11T07:51:16.042995Z","iopub.status.idle":"2021-12-11T07:51:16.048659Z","shell.execute_reply.started":"2021-12-11T07:51:16.042955Z","shell.execute_reply":"2021-12-11T07:51:16.047773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_and_fix(img_path, img_name):\n    # detect for premature ending\n    try:\n        with open( img_path, 'rb') as im :\n            im.seek(-2,2)\n            if im.read() == b'\\xff\\xd9':\n                print('Image OK :', img_name) \n            else: \n                # fix image\n                img = cv2.imread(img_path)\n                cv2.imwrite( img_path, img)\n                print('FIXED corrupted image :', img_name)           \n    except(IOError, SyntaxError) as e :\n      print(e)\n      print(\"Unable to load/write Image : {} . Image might be destroyed\".format(img_path) )\n\n\nfor path in os.listdir(dir_path):\n    # Make sure to change the extension if it is nor 'jpg' ( for example 'JPG','PNG' etc..)\n    if path.endswith('.jpg'):\n      img_path = os.path.join(dir_path, path)\n      detect_and_fix( img_path=img_path, img_name = path)\n\nprint(\"Process Finished\")","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:16.050605Z","iopub.execute_input":"2021-12-11T07:51:16.050875Z","iopub.status.idle":"2021-12-11T07:51:17.492851Z","shell.execute_reply.started":"2021-12-11T07:51:16.050838Z","shell.execute_reply":"2021-12-11T07:51:17.492143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checked all 3 folders for corrupted files.They are all clean.","metadata":{}},{"cell_type":"code","source":"# create dataframe of file and labels\nfiles = {'filepath': type1_files + type2_files + type3_files,\n          'label': ['Type 1']* len(type1_files) + ['Type 2']* len(type2_files) + ['Type 3']* len(type3_files)}\n\nfiles_df = pd.DataFrame(files).sample(frac=1, random_state= 1).reset_index(drop=True)\nfiles_df['filepath'][10]\nfiles_df","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:17.496141Z","iopub.execute_input":"2021-12-11T07:51:17.496361Z","iopub.status.idle":"2021-12-11T07:51:17.516619Z","shell.execute_reply.started":"2021-12-11T07:51:17.496332Z","shell.execute_reply":"2021-12-11T07:51:17.515943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning and Exploration","metadata":{}},{"cell_type":"code","source":"# describe the dataframe\nfiles_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:17.518454Z","iopub.execute_input":"2021-12-11T07:51:17.518858Z","iopub.status.idle":"2021-12-11T07:51:17.535098Z","shell.execute_reply.started":"2021-12-11T07:51:17.518822Z","shell.execute_reply":"2021-12-11T07:51:17.534428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for duplicates\nlen(files_df[files_df.duplicated(subset=['filepath'])])\n#print(files_df['filepath'].duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:17.536492Z","iopub.execute_input":"2021-12-11T07:51:17.53703Z","iopub.status.idle":"2021-12-11T07:51:17.545064Z","shell.execute_reply.started":"2021-12-11T07:51:17.536989Z","shell.execute_reply":"2021-12-11T07:51:17.544308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for damaged files\nbad_files = []\nfor path in (files_df['filepath'].values):\n    try:\n        img = Image.open(path)\n    except:\n        index = files_df[files_df['filepath']==path].index.values[0]\n        bad_files.append(index)\nprint(len(bad_files))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:18.160862Z","iopub.execute_input":"2021-12-11T07:51:18.161675Z","iopub.status.idle":"2021-12-11T07:51:35.200761Z","shell.execute_reply.started":"2021-12-11T07:51:18.16162Z","shell.execute_reply":"2021-12-11T07:51:35.199955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # show the bad files\nprint(bad_files)\n# drop the damaged files\nfiles_df.drop(bad_files, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:35.202488Z","iopub.execute_input":"2021-12-11T07:51:35.202748Z","iopub.status.idle":"2021-12-11T07:51:35.211752Z","shell.execute_reply.started":"2021-12-11T07:51:35.202711Z","shell.execute_reply":"2021-12-11T07:51:35.210962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check length of files in dataframe\nlen(files_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:35.213105Z","iopub.execute_input":"2021-12-11T07:51:35.213471Z","iopub.status.idle":"2021-12-11T07:51:35.222852Z","shell.execute_reply.started":"2021-12-11T07:51:35.213433Z","shell.execute_reply":"2021-12-11T07:51:35.222003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check unique labels\nfiles_df['label'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:35.225822Z","iopub.execute_input":"2021-12-11T07:51:35.226138Z","iopub.status.idle":"2021-12-11T07:51:35.233265Z","shell.execute_reply.started":"2021-12-11T07:51:35.226103Z","shell.execute_reply":"2021-12-11T07:51:35.23233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get count of each type \ntype_count = pd.DataFrame(files_df['label'].value_counts()).rename(columns= {'label': 'Num_Values'})\ntype_count","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:35.234944Z","iopub.execute_input":"2021-12-11T07:51:35.235207Z","iopub.status.idle":"2021-12-11T07:51:35.25009Z","shell.execute_reply.started":"2021-12-11T07:51:35.235175Z","shell.execute_reply":"2021-12-11T07:51:35.249244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display barplot of type count\nplt.figure(figsize = (15, 6))\nsns.barplot(x= type_count['Num_Values'], y= type_count.index.to_list())\nplt.title('Cervical Cancer Type Distribution')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:35.251433Z","iopub.execute_input":"2021-12-11T07:51:35.25168Z","iopub.status.idle":"2021-12-11T07:51:35.443106Z","shell.execute_reply.started":"2021-12-11T07:51:35.251647Z","shell.execute_reply":"2021-12-11T07:51:35.442453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The data distribution plot shows that type 1 class has more datapoints than other types, with type 3 having the least datapoints.\n\n### A pie plot is useful in visualizing the percentage of data distribution.\n","metadata":{}},{"cell_type":"code","source":"# display pieplot of label distribution\npie_plot = go.Pie(labels= type_count.index.to_list(), values= type_count.values.flatten(),\n                 hole= 0.2, text= type_count.index.to_list(), textposition='auto')\nfig = go.Figure([pie_plot])\nfig.update_layout(title_text='Pie Plot of Type Distribution')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:35.444472Z","iopub.execute_input":"2021-12-11T07:51:35.444723Z","iopub.status.idle":"2021-12-11T07:51:35.456723Z","shell.execute_reply.started":"2021-12-11T07:51:35.444689Z","shell.execute_reply":"2021-12-11T07:51:35.455949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display sample images of types\nfor label in ('Type 1', 'Type 2', 'Type 3'):\n    filepaths = files_df[files_df['label']==label]['filepath'].values[:5]\n    fig = plt.figure(figsize= (15, 6))\n    for i, path in enumerate(filepaths):\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        img = cv2.resize(img, (224, 224))\n        fig.add_subplot(1, 5, i+1)\n        plt.imshow(img)\n        plt.subplots_adjust(hspace=0.5)\n        plt.axis(False)\n        plt.title(label)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:35.457801Z","iopub.execute_input":"2021-12-11T07:51:35.458213Z","iopub.status.idle":"2021-12-11T07:51:39.986607Z","shell.execute_reply.started":"2021-12-11T07:51:35.458174Z","shell.execute_reply":"2021-12-11T07:51:39.985876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"code","source":"\ndef load_images(dataframe):\n    features = []\n    filepaths = dataframe['filepath'].values\n    labels = dataframe['label'].values\n    \n    for path in filepaths:\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        img = cv2.resize(img, (224, 224))\n        img = cv2.GaussianBlur(img , (5,5) ,0)\n        features.append(np.array(img))\n    return np.array(features), np.array(labels)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:39.988086Z","iopub.execute_input":"2021-12-11T07:51:39.988592Z","iopub.status.idle":"2021-12-11T07:51:39.995587Z","shell.execute_reply.started":"2021-12-11T07:51:39.988554Z","shell.execute_reply":"2021-12-11T07:51:39.994762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  split the data into train  and validation set\ntrain_df, eval_df = train_test_split(files_df, test_size= 0.3, stratify= files_df['label'], random_state= 1)\nval_df, test_df = train_test_split(eval_df, test_size= 0.2, stratify= eval_df['label'], random_state= 1)\nprint(len(train_df), len(val_df), len(test_df))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:39.998175Z","iopub.execute_input":"2021-12-11T07:51:39.998711Z","iopub.status.idle":"2021-12-11T07:51:40.016901Z","shell.execute_reply.started":"2021-12-11T07:51:39.998674Z","shell.execute_reply":"2021-12-11T07:51:40.016089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load training and evaluation data\ntrain_features, train_labels = load_images(train_df)\nval_features, val_labels = load_images(val_df)\ntest_features, test_labels = load_images(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:51:40.018287Z","iopub.execute_input":"2021-12-11T07:51:40.018528Z","iopub.status.idle":"2021-12-11T07:57:36.043602Z","shell.execute_reply.started":"2021-12-11T07:51:40.018495Z","shell.execute_reply":"2021-12-11T07:57:36.041573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check lengths of training and evaluation  sets\nlen(train_features), len(train_labels), len(test_features), len(test_labels), len(val_features), len(val_labels) ","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:57:36.047487Z","iopub.execute_input":"2021-12-11T07:57:36.048578Z","iopub.status.idle":"2021-12-11T07:57:36.057707Z","shell.execute_reply.started":"2021-12-11T07:57:36.048521Z","shell.execute_reply":"2021-12-11T07:57:36.057027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get image shape\nInputShape = train_features[0].shape\nprint(InputShape)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:57:36.05883Z","iopub.execute_input":"2021-12-11T07:57:36.059416Z","iopub.status.idle":"2021-12-11T07:57:36.072416Z","shell.execute_reply.started":"2021-12-11T07:57:36.059378Z","shell.execute_reply":"2021-12-11T07:57:36.071652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_cols,img_rows = 224,224\ntrain_features= train_features.reshape(train_features.shape[0], img_cols*img_rows*3)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:57:36.073599Z","iopub.execute_input":"2021-12-11T07:57:36.074112Z","iopub.status.idle":"2021-12-11T07:57:36.081338Z","shell.execute_reply.started":"2021-12-11T07:57:36.074073Z","shell.execute_reply":"2021-12-11T07:57:36.080434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalize the features\nX_train = train_features/255\nX_val  = val_features/255\nX_test  = test_features/255","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:57:36.082599Z","iopub.execute_input":"2021-12-11T07:57:36.083444Z","iopub.status.idle":"2021-12-11T07:57:37.806145Z","shell.execute_reply.started":"2021-12-11T07:57:36.083405Z","shell.execute_reply":"2021-12-11T07:57:37.805327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train_labels\ny_test = test_labels\ny_val = val_labels","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:57:37.808813Z","iopub.execute_input":"2021-12-11T07:57:37.809106Z","iopub.status.idle":"2021-12-11T07:57:37.814169Z","shell.execute_reply.started":"2021-12-11T07:57:37.809049Z","shell.execute_reply":"2021-12-11T07:57:37.813383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:57:37.815309Z","iopub.execute_input":"2021-12-11T07:57:37.815663Z","iopub.status.idle":"2021-12-11T07:57:37.826752Z","shell.execute_reply.started":"2021-12-11T07:57:37.815632Z","shell.execute_reply":"2021-12-11T07:57:37.825972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:57:37.828046Z","iopub.execute_input":"2021-12-11T07:57:37.828881Z","iopub.status.idle":"2021-12-11T07:57:37.83571Z","shell.execute_reply.started":"2021-12-11T07:57:37.828815Z","shell.execute_reply":"2021-12-11T07:57:37.83472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\ncounter = Counter(y_train)\nprint(counter)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:57:37.837038Z","iopub.execute_input":"2021-12-11T07:57:37.837782Z","iopub.status.idle":"2021-12-11T07:57:37.848028Z","shell.execute_reply.started":"2021-12-11T07:57:37.837748Z","shell.execute_reply":"2021-12-11T07:57:37.84717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n#x_train = pd.DataFrame(X_train)\nX_resample, y_resampled = SMOTE().fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:57:37.849437Z","iopub.execute_input":"2021-12-11T07:57:37.850288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_resample = X_resample.reshape(X_resample.shape[0], img_cols,img_rows,3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_resample.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\ncounter = Counter(y_resampled)\nprint(counter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nNUM_CLASSES = 3\nEPOCHS = 50\nINPUT_SHAPE = (224, 224, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder().fit(['Type 1', 'Type 2', 'Type 3'])\nY_train = le.transform(y_resampled)\nY_val = le.transform(y_val)\nY_test = le.transform(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize image data generator for training and evaluation sets\n\ntrain_datagen = ImageDataGenerator(\n                                rotation_range = 40,\n                                zoom_range = 0.2,\n                                width_shift_range=0.2,\n                                height_shift_range=0.2,\n                                shear_range=0.2,\n                                horizontal_flip=True,\n                                vertical_flip = True)\ntest_datagen = ImageDataGenerator()\neval_datagen = ImageDataGenerator()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply data augmentation to features\nBATCH_SIZE= 32\ntrain_gen = train_datagen.flow(X_resample, Y_train, batch_size= BATCH_SIZE)\nval_gen = eval_datagen.flow(X_val, Y_val, batch_size= BATCH_SIZE)\ntest_gen = eval_datagen.flow(X_test, Y_test, batch_size= BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show shape of each  batch\nfor data_batch, labels_batch in train_gen:\n    print('data batch shape: {} \\n labels batch shape: {}'.format(data_batch.shape, labels_batch.shape))\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model building","metadata":{}},{"cell_type":"code","source":"# initialize pretrained vgg model base\nconv_base = VGG16(weights= 'imagenet', include_top= False, input_shape= (224, 224, 3))\nconv_base.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show trainable layers before freezing\nprint('This is the number of trainable weights '\n'before freezing layers in the conv base:', len(conv_base.trainable_weights))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# freeze few layers of pretrained model\nfor layer in conv_base.layers[:-4]:\n    layer.trainable= False\n    # Check the trainable status of the individual layers\nfor layer in conv_base.layers:\n    print(layer, layer.trainable)\nconv_base.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show trainable layers after freezing\nprint('This is the number of trainable weights '\n'after freezing layers in the conv base:', len(conv_base.trainable_weights))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build model \nmodel = Sequential([conv_base, \n                    Flatten(),\n                    Dense(1024, activation='relu'),\n                   Dropout(0.5),\n                   Dense(3, activation='softmax')])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compile model\nmodel.compile(optimizer= Adam(0.0001), loss= 'sparse_categorical_crossentropy', metrics= ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show model summary\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define training steps\nTRAIN_STEPS = len(train_df)//BATCH_SIZE\nVAL_STEPS = len(val_df)//BATCH_SIZE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduceLR = ReduceLROnPlateau(monitor='val_loss', patience=10, verbose= 1, mode='min', factor=  0.2, min_lr = 1e-5)\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience = 20, verbose=1, mode='min', restore_best_weights= True)\n\ncheckpoint = ModelCheckpoint('cervicalModel_noaug.weights.hdf5', monitor='val_loss', verbose=1,save_best_only=True, mode= 'min')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model\nhistory = model.fit(train_gen, steps_per_epoch= TRAIN_STEPS, validation_data=val_gen, validation_steps=VAL_STEPS, epochs= 50,\n                   callbacks= [reduceLR, early_stopping, checkpoint])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T06:39:15.852691Z","iopub.execute_input":"2021-12-11T06:39:15.853488Z","iopub.status.idle":"2021-12-11T06:40:01.991811Z","shell.execute_reply.started":"2021-12-11T06:39:15.85345Z","shell.execute_reply":"2021-12-11T06:40:01.989409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read training history into dataframe\nhistory_df = pd.DataFrame(history.history)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:29:07.601282Z","iopub.execute_input":"2021-12-07T05:29:07.601575Z","iopub.status.idle":"2021-12-07T05:29:07.608237Z","shell.execute_reply.started":"2021-12-07T05:29:07.601544Z","shell.execute_reply":"2021-12-07T05:29:07.60753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display training and validation history\n\n# display history of accurracy\nplt.figure(figsize= (15,6))\nplt.subplot(1,2,1)\nplt.plot(history_df['accuracy'], label= 'accuracy' )\nplt.plot(history_df['val_accuracy'], label= 'val_accuracy')\n# history_df[['acc', 'val_acc']]\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy History')\nplt.legend()\n\n# display history of loss\nplt.subplot(1,2,2)\nplt.plot(history_df['loss'], label= 'loss')\nplt.plot(history_df['val_loss'], label= 'val_loss')\n# history_df[['loss', 'val_loss']].plot()\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss History')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:29:08.29589Z","iopub.execute_input":"2021-12-07T05:29:08.296677Z","iopub.status.idle":"2021-12-07T05:29:08.690008Z","shell.execute_reply.started":"2021-12-07T05:29:08.296635Z","shell.execute_reply":"2021-12-07T05:29:08.689293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# load best weights into model\nmodel.load_weights('./cervicalModel_aug_moredata.weights.hdf5')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T06:41:39.18936Z","iopub.execute_input":"2021-12-11T06:41:39.18998Z","iopub.status.idle":"2021-12-11T06:41:39.526866Z","shell.execute_reply.started":"2021-12-11T06:41:39.189941Z","shell.execute_reply":"2021-12-11T06:41:39.526088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model on test set\nmodel.evaluate(test_gen)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T06:41:44.625251Z","iopub.execute_input":"2021-12-11T06:41:44.625807Z","iopub.status.idle":"2021-12-11T06:41:44.956796Z","shell.execute_reply.started":"2021-12-11T06:41:44.625769Z","shell.execute_reply":"2021-12-11T06:41:44.956081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-12-11T06:41:50.557145Z","iopub.execute_input":"2021-12-11T06:41:50.557682Z","iopub.status.idle":"2021-12-11T06:41:50.562307Z","shell.execute_reply.started":"2021-12-11T06:41:50.557645Z","shell.execute_reply":"2021-12-11T06:41:50.561531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x = model.predict(X_test) \n#print(predict_x)\ny_pred = np.argmax(predict_x,axis=1)\ny_pred = np.array(y_pred)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2021-12-11T06:41:54.931426Z","iopub.execute_input":"2021-12-11T06:41:54.931687Z","iopub.status.idle":"2021-12-11T06:41:55.496819Z","shell.execute_reply.started":"2021-12-11T06:41:54.931658Z","shell.execute_reply":"2021-12-11T06:41:55.496128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = Y_test\ny_true","metadata":{"execution":{"iopub.status.busy":"2021-12-11T06:42:09.249187Z","iopub.execute_input":"2021-12-11T06:42:09.24989Z","iopub.status.idle":"2021-12-11T06:42:09.257352Z","shell.execute_reply.started":"2021-12-11T06:42:09.249852Z","shell.execute_reply":"2021-12-11T06:42:09.25616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(y_true,y_pred)\nprint('Accuracy: %f' % accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T06:42:12.147579Z","iopub.execute_input":"2021-12-11T06:42:12.148048Z","iopub.status.idle":"2021-12-11T06:42:12.153655Z","shell.execute_reply.started":"2021-12-11T06:42:12.148011Z","shell.execute_reply":"2021-12-11T06:42:12.152828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision = precision_score(y_true,y_pred, average='micro')\nprint('Precision: %f' % precision)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T06:42:14.618887Z","iopub.execute_input":"2021-12-11T06:42:14.619351Z","iopub.status.idle":"2021-12-11T06:42:14.630133Z","shell.execute_reply.started":"2021-12-11T06:42:14.61929Z","shell.execute_reply":"2021-12-11T06:42:14.629253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recall = recall_score(y_true,y_pred, average='micro')\nprint('Recall: %f' % recall)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T06:44:39.145022Z","iopub.execute_input":"2021-12-11T06:44:39.145331Z","iopub.status.idle":"2021-12-11T06:44:39.153248Z","shell.execute_reply.started":"2021-12-11T06:44:39.145294Z","shell.execute_reply":"2021-12-11T06:44:39.152314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_true,y_pred)\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(y_true,y_pred, average='weighted')\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_true,y_pred, average='weighted')\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(y_true,y_pred, average='weighted')\nprint('F1 score: %f' % f1)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T06:44:41.992929Z","iopub.execute_input":"2021-12-11T06:44:41.993394Z","iopub.status.idle":"2021-12-11T06:44:42.006921Z","shell.execute_reply.started":"2021-12-11T06:44:41.993356Z","shell.execute_reply":"2021-12-11T06:44:42.006131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix for actual and predicted values.\nmatrix = confusion_matrix(y_true,y_pred, labels=[0,1,2])\nprint('Confusion matrix :')\nprint(matrix)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T06:44:55.544679Z","iopub.execute_input":"2021-12-11T06:44:55.544934Z","iopub.status.idle":"2021-12-11T06:44:55.554072Z","shell.execute_reply.started":"2021-12-11T06:44:55.544906Z","shell.execute_reply":"2021-12-11T06:44:55.553164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}